{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The goal of this notebook is to cluster and assign cluster numbers to each word so that they might be useful as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported custom BASIC modules\n"
     ]
    }
   ],
   "source": [
    "import basic\n",
    "from basic.nlp.tokenizers import clinical_tokenizers\n",
    "from basic.nlp.annotation.annotation import Annotation, AnnotatedDocument\n",
    "from basic.nlp.sequenceutils import get_sentence_bio_tagged_tokens\n",
    "\n",
    "from madetokenizer import build_made_tokenizer\n",
    "from madeutils import read_made_data, get_all_sentence_tokens_and_tags, gather_validation_metrics\n",
    "\n",
    "print('Imported custom BASIC modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_BASE_DIR = r'c:\\embeddings'\n",
    "\n",
    "CLUSTERS_BASE_DIR = r'resources/clusters'\n",
    "\n",
    "PRETRAINED_EMBEDDINGS_FILENAME = r'wikipedia-pubmed-and-PMC-w2v.bin'\n",
    "#PRETRAINED_EMBEDDINGS_FILENAME = r'pubmed+wiki+pitts-nopunct-lower-cbow-n10.bin'\n",
    "\n",
    "K_CLUSTERS = 10000\n",
    "ENABLED_BATCH_KMEANS = True\n",
    "KMEANS_BATCH_SIZE = 500000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load our embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyedVectors<vector_size=200, 5443656 keys>\n"
     ]
    }
   ],
   "source": [
    "# let's load some pretrained embeddings as well\n",
    "\n",
    "# NOTE : These embeddings are made available here:\n",
    "# http://evexdb.org/pmresources/vec-space-models/\n",
    "#Aashri Note: I hard coded the directory path to this embeddings file instead of using the above global variables as seen in the original code\n",
    "pretrained_word_vectors = KeyedVectors.load_word2vec_format(r\"C:\\Users\\aaa4026\\Desktop\\MADE-CRF-master\\embeddings\\wikipedia-pubmed-and-PMC-w2v.bin\", binary=True)  # C binary format\n",
    "                                                 \n",
    "print(pretrained_word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "print(pretrained_word_vectors['the'].shape)\n",
    "\n",
    "pretrained_embeddings_dimensions = pretrained_word_vectors['the'].shape[0]\n",
    "print(pretrained_embeddings_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pretrained_word_vectors\n",
    "embeddings_dimensions = pretrained_embeddings_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running K means\n",
      "Using batch KMeans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaa4026\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1930: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#changed from .syn0 to .vectors due to .syn0 being depricated in the recent version of the package\n",
    "word_vectors = pretrained_word_vectors.vectors\n",
    "\n",
    "# Initalize a k-means object and use it to extract centroids\n",
    "print('Running K means')\n",
    "\n",
    "if ENABLED_BATCH_KMEANS:\n",
    "    print('Using batch KMeans')\n",
    "    kmeans = MiniBatchKMeans(n_clusters = K_CLUSTERS, \n",
    "                         #n_jobs = -2, \n",
    "                         batch_size = KMEANS_BATCH_SIZE)\n",
    "else:\n",
    "    print('Using original recipe KMeans')\n",
    "    kmeans = KMeans( n_clusters = K_CLUSTERS, n_jobs = -2 )\n",
    "\n",
    "cluster_idx = kmeans.fit_predict( word_vectors )\n",
    "\n",
    "print('K means trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Word / Index dictionary, mapping each vocabulary word to\n",
    "# a cluster number\n",
    "#Aashri edit: took out .wv\n",
    "word_cluster_map = dict(zip(pretrained_word_vectors.index_to_key, cluster_idx ))\n",
    "\n",
    "print(list(word_cluster_map.items())[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word_cluster_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typename = 'KMeans'\n",
    "if ENABLED_BATCH_KMEANS:\n",
    "    typename = 'BatchKmeans'\n",
    "\n",
    "map_pickle_file_name = '{3}/WordClusters_K{0}_{1}_{2}.pickle'.format(K_CLUSTERS, \n",
    "                                                                     typename, \n",
    "                                                                     PRETRAINED_EMBEDDINGS_FILENAME.split('.')[0],\n",
    "                                                                    CLUSTERS_BASE_DIR)\n",
    "\n",
    "print('Writing cluster map pickle to : {}'.format(map_pickle_file_name))\n",
    "      \n",
    "with open(map_pickle_file_name, 'wb') as handle:\n",
    "    pickle.dump(word_cluster_map, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "      \n",
    "print('DONE writing cluster map pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
